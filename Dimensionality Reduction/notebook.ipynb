{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Dimensionality Reduction\n",
    "\n",
    "Many machine learning problems involve thousands or even millions of\n",
    "features for each training instance. Not only do all these features make\n",
    "training extremely slow, but they can also make it much harder to find a good\n",
    "solution, as you will see.\n",
    "\n",
    "Apart from speeding up training, dimensionality reduction is also extremely\n",
    "useful for data visualization. Reducing the number of dimensions down to\n",
    "two (or three) makes it possible to plot a condensed view of a \n",
    "high-dimensional training set on a graph and often gain some important insights by\n",
    "visually detecting patterns, such as clusters. Moreover, data visualization is\n",
    "essential to communicate your conclusions to people who are not data\n",
    "scientists—in particular, decision makers who will use your results.\n",
    "\n",
    "## The Curse of Dimensionality\n",
    "\n",
    "We are so used to living in three dimensions that our intuition fails us\n",
    "when we try to imagine a high-dimensional space. It turns out that many\n",
    "things behave very differently in high-dimensional space.\n",
    "\n",
    "Here is a more troublesome difference: if you pick two points randomly in a\n",
    "unit square, the distance between these two points will be, on average,\n",
    "roughly 0.52. If you pick two random points in a 3D unit cube, the average\n",
    "distance will be roughly 0.66. But what about two points picked randomly in\n",
    "a 1,000,000-dimensional unit hypercube? The average distance, believe it or\n",
    "not, will be about 408.25 (roughly 1,000,0006)! This is counterintuitive: how\n",
    "can two points be so far apart when they both lie within the same unit\n",
    "hypercube? Well, there’s just plenty of space in high dimensions. As a result,\n",
    "high-dimensional datasets are at risk of being very sparse: most training\n",
    "instances are likely to be far away from each other. This also means that a\n",
    "new instance will likely be far away from any training instance, making\n",
    "predictions much less reliable than in lower dimensions, since they will be\n",
    "based on much larger extrapolations. In short, the more dimensions the\n",
    "training set has, the greater the risk of overfitting it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Approaches for Dimensionality Reduction\n",
    "\n",
    "Before we dive into specific dimensionality reduction algorithms, let’s take a\n",
    "look at the two main approaches to reducing dimensionality: projection and\n",
    "manifold learning.\n",
    "\n",
    "### Projection\n",
    "\n",
    "In most real-world problems, training instances are not spread out uniformly\n",
    "across all dimensions. Many features are almost constant, while others are\n",
    "highly correlated (as discussed earlier for MNIST). As a result, all training\n",
    "instances lie within (or close to) a much lower-dimensional subspace of the\n",
    "high-dimensional space.\n",
    "\n",
    "### Manifold Learning\n",
    "\n",
    "However, projection is not always the best approach to dimensionality\n",
    "reduction. In many cases the subspace may twist and turn, such as in the\n",
    "famous Swiss roll toy dataset.\n",
    "\n",
    "Many dimensionality reduction algorithms work by modeling the manifold\n",
    "on which the training instances lie; this is called manifold learning. It relies\n",
    "on the manifold assumption, also called the manifold hypothesis, which holds\n",
    "that most real-world high-dimensional datasets lie close to a much \n",
    "lower-dimensional manifold. This assumption is very often empirically observed.\n",
    "\n",
    "In short, reducing the dimensionality of your training set before training a\n",
    "model will usually speed up training, but it may not always lead to a better or\n",
    "simpler solution; it all depends on the dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "Principal component analysis (PCA) is by far the most popular\n",
    "dimensionality reduction algorithm. First it identifies the hyperplane that lies\n",
    "closest to the data, and then it projects the data onto it.\n",
    "\n",
    "### Preserving the Variance\n",
    "It seems reasonable to select the axis that preserves the maximum amount of\n",
    "variance, as it will most likely lose less information than the other\n",
    "projections. Another way to justify this choice is that it is the axis that\n",
    "minimizes the mean squared distance between the original dataset and its\n",
    "projection onto that axis. This is the rather simple idea behind PCA.\n",
    "\n",
    "### Pricipal Components\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the\n",
    "training set. It also finds a second axis,\n",
    "orthogonal to the first one, that accounts for the largest amount of the\n",
    "remaining variance. In this 2D example there is no choice: it is the dotted\n",
    "line. If it were a higher-dimensional dataset, PCA would also find a third\n",
    "axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as\n",
    "many axes as the number of dimensions in the dataset.\n",
    "\n",
    "The $i^th$ axis is called the $i^th$ principal component (PC) of the data.\n",
    "\n",
    "So how can you find the principal components of a training set? Luckily,\n",
    "there is a standard matrix factorization technique called singular value\n",
    "decomposition (SVD) that can decompose the training set matrix X into the\n",
    "matrix multiplication of three matrices U Σ V , where V contains the unit\n",
    "vectors that define all the principal components that you are looking for.\n",
    "\n",
    "### Projecting Down to d Dimensions\n",
    "Once you have identified all the principal components, you can reduce the\n",
    "dimensionality of the dataset down to d dimensions by projecting it onto the\n",
    "hyperplane defined by the first d principal components. Selecting this\n",
    "hyperplane ensures that the projection will preserve as much variance as\n",
    "possible.\n",
    "\n",
    "### Explained Variance Ratio\n",
    "Another useful piece of information is the **explained variance ratio** of each\n",
    "principal component, available via the explained_variance_ratio_ variable.\n",
    "The ratio indicates the *proportion of the dataset’s variance* that lies along\n",
    "each principal component.\n",
    "\n",
    "### Choosing the Right Number of Dimensions\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to,\n",
    "it is simpler to choose the number of dimensions that add up to a sufficiently\n",
    "large portion of the variance—say, 95% (An exception to this rule, of course,\n",
    "is if you are reducing dimensionality for data visualization, in which case you\n",
    "will want to reduce the dimensionality down to 2 or 3).\n",
    "\n",
    "### PCA for Compression\n",
    "After dimensionality reduction, the training set takes up much less space. For\n",
    "example, after applying PCA to the MNIST dataset while preserving 95% of\n",
    "its variance, we are left with 154 features, instead of the original 784 features.\n",
    "So the dataset is now less than 20% of its original size, and we only lost 5%\n",
    "of its variance! This is a reasonable compression ratio, and it’s easy to see\n",
    "how such a size reduction would speed up a classification algorithm\n",
    "tremendously.\n",
    "It is also possible to decompress the reduced dataset back to 784 dimensions\n",
    "by applying the inverse transformation of the PCA projection. This won’t\n",
    "give you back the original data, since the projection lost a bit of information\n",
    "(within the 5% variance that was dropped), but it will likely be close to the\n",
    "original data. The mean squared distance between the original data and the\n",
    "reconstructed data (compressed and then decompressed) is called the\n",
    "reconstruction error.\n",
    "\n",
    "### Randomized PCA\n",
    "If you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses\n",
    "a stochastic algorithm called randomized PCA that quickly finds an\n",
    "approximation of the first d principal components. Its computational\n",
    "complexity is O(m × d ) + O(d ), instead of O(m × n ) + O(n ) for the full\n",
    "SVD approach, so it is dramatically faster than full SVD when d is much\n",
    "smaller than n.\n",
    "\n",
    "### Incremental PCA\n",
    "One problem with the preceding implementations of PCA is that they require\n",
    "the whole training set to fit in memory in order for the algorithm to run.\n",
    "Fortunately, incremental PCA (IPCA) algorithms have been developed that\n",
    "allow you to split the training set into mini-batches and feed these in one\n",
    "mini-batch at a time. This is useful for large training sets and for applying\n",
    "PCA online (i.e., on the fly, as new instances arrive)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Projection\n",
    "As its name suggests, the random projection algorithm projects the data to a\n",
    "lower-dimensional space using a random linear projection. This may sound\n",
    "crazy, but it turns out that such a random projection is actually very likely to\n",
    "preserve distances fairly well, as was demonstrated mathematically by\n",
    "William B. Johnson and Joram Lindenstrauss in a famous lemma. So, two\n",
    "similar instances will remain similar after the projection, and two very\n",
    "different instances will remain very different.\n",
    "\n",
    "Obviously, the more dimensions you drop, the more information is lost, and\n",
    "the more distances get distorted. So how can you choose the optimal number\n",
    "of dimensions? Well, Johnson and Lindenstrauss came up with an equation\n",
    "that determines the minimum number of dimensions to preserve in order to\n",
    "ensure—with high probability—that distances won’t change by more than a\n",
    "given tolerance. For example, if you have a dataset containing m = 5,000\n",
    "instances with n = 20,000 features each, and you don’t want the squared\n",
    "distance between any two instances to change by more than ε = 10%, then\n",
    "you should project the data down to d dimensions, with d ≥ 4 log(m) / (½ ε² -\n",
    "⅓ ε³), which is 7,300 dimensions. That’s quite a significant dimensionality\n",
    "reduction! Notice that the equation does not use n, it only relies on m and ε.\n",
    "\n",
    "In summary, random projection is a simple, fast, memory-efficient, and\n",
    "surprisingly powerful dimensionality reduction algorithm that you should\n",
    "keep in mind, especially when you deal with high-dimensional datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLE\n",
    "Locally linear embedding (LLE) is a nonlinear dimensionality reduction\n",
    "(NLDR) technique. It is a manifold learning technique that does not rely on\n",
    "projections, unlike PCA and random projection. In a nutshell, LLE works by\n",
    "first measuring how each training instance linearly relates to its nearest\n",
    "neighbors, and then looking for a low-dimensional representation of the\n",
    "training set where these local relationships are best preserved (more details\n",
    "shortly). This approach makes it particularly good at unrolling twisted\n",
    "manifolds, especially when there is not too much noise.\n",
    "\n",
    "Here’s how LLE works: for each training instance x , the algorithm\n",
    "identifies its k-nearest neighbors (in the preceding code k = 10), then tries to\n",
    "reconstruct x as a linear function of these neighbors.\n",
    "\n",
    "After this step, the weight matrix W ^ (containing the weights w^i,j) encodes\n",
    "the local linear relationships between the training instances. The second step\n",
    "is to map the training instances into a d-dimensional space (where d < n)\n",
    "while preserving these local relationships as much as possible.\n",
    "\n",
    "As you can see, LLE is quite different from the projection techniques, and it’s\n",
    "significantly more complex, but it can also construct much better \n",
    "low-dimensional representations, especially if the data is nonlinear."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
