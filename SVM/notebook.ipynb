{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "A support vector machine (SVM) is a powerful and versatile machine\n",
    "learning model, capable of performing linear or nonlinear classification,\n",
    "regression, and even novelty detection. SVMs shine with small to medium￾sized nonlinear datasets (i.e., hundreds to thousands of instances), especially\n",
    "for classification tasks. However, they don’t scale very well to very large\n",
    "datasets, as you will see."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM Classification\n",
    "\n",
    "Linear SVM (Support Vector Machine) classification is a supervised learning algorithm used for binary classification problems. The goal of the algorithm is to find a hyperplane that separates the two classes of data points with the largest margin possible.\n",
    "\n",
    "SVM are sensitive to the feature scales.\n",
    "\n",
    "Unlike LogisticRegression, LinearSVC doesn’t have a predict_proba()\n",
    "method to estimate the class probabilities. That said, if you use the SVC class\n",
    "(discussed shortly) instead of LinearSVC, and if you set its probability\n",
    "hyperparameter to True, then the model will fit an extra model at the end of\n",
    "training to map the SVM decision function scores to estimated probabilities.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin Classification\n",
    "\n",
    "If we strictly impose that all instances must be off the street and on the\n",
    "correct side, this is called **hard margin classification**. There are two main\n",
    "issues with hard margin classification. First, it only works if the data is\n",
    "linearly separable. Second, it is sensitive to outliers.\n",
    "\n",
    "To avoid these issues, we need to use a more flexible model. The objective is\n",
    "to find a good balance between keeping the street as large as possible and\n",
    "limiting the margin violations (i.e., instances that end up in the middle of the\n",
    "street or even on the wrong side). This is called **soft margin classification**.\n",
    "\n",
    "To do that, we can specify hyperparameter C. Reducing C makes the street larger, \n",
    "but it also leads to more margin violations. In other\n",
    "words, reducing C results in more instances supporting the street, so there’s\n",
    "less risk of overfitting. But if you reduce it too much, then the model ends up\n",
    "underfitting, as seems to be the case here: the model with C=100 looks like it\n",
    "will generalize better than the one with C=1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "\n",
    "Although linear SVM classifiers are efficient and often work surprisingly\n",
    "well, many datasets are not even close to being linearly separable. One\n",
    "approach to handling nonlinear datasets is to add more features, such as\n",
    "polynomial features.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel\n",
    "\n",
    "Adding polynomial features is simple to implement and can work great with\n",
    "all sorts of machine learning algorithms (not just SVMs). That said, at a low\n",
    "polynomial degree this method cannot deal with very complex datasets, and\n",
    "with a high polynomial degree it creates a huge number of features, making\n",
    "the model too slow.\n",
    "\n",
    "Fortunately, when using SVMs you can apply an almost miraculous\n",
    "mathematical technique called the kernel trick (which is explained later in\n",
    "this chapter). The kernel trick makes it possible to get the same result as if\n",
    "you had added many polynomial features, even with a very high degree,\n",
    "without actually having to add them. This means there’s no combinatorial\n",
    "explosion of the number of features. This trick is implemented by the SVC\n",
    "class."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "\n",
    "Another technique to tackle nonlinear problems is to add features computed\n",
    "using a similarity function, which measures how much each instance\n",
    "resembles a particular landmark.\n",
    "\n",
    "You may wonder how to select the landmarks. The simplest approach is to\n",
    "create a landmark at the location of each and every instance in the dataset.\n",
    "Doing that creates many dimensions and thus increases the chances that the\n",
    "transformed training set will be linearly separable. The downside is that a\n",
    "training set with m instances and n features gets transformed into a training\n",
    "set with m instances and m features (assuming you drop the original features).\n",
    "If your training set is very large, you end up with an equally large number of\n",
    "features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "\n",
    "Just like the polynomial features method, the similarity features method can\n",
    "be useful with any machine learning algorithm, but it may be computationally\n",
    "expensive to compute all the additional features (especially on large training\n",
    "sets). Once again the kernel trick does its SVM magic, making it possible to\n",
    "obtain a similar result as if you had added many similarity features, but\n",
    "without actually doing so.\n",
    "\n",
    "Models trained with different values of hyperparameters gamma (γ) and\n",
    "C. Increasing gamma makes the bell-shaped curve narrower. \n",
    "Conversely, a small gamma value makes the bell-shaped\n",
    "curve wider: instances have a larger range of influence, and the decision\n",
    "boundary ends up smoother. So γ acts like a regularization hyperparameter: if\n",
    "your model is overfitting, you should reduce γ; if it is underfitting, you should\n",
    "increase γ (similar to the C hyperparameter)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Regression\n",
    "\n",
    "To use SVMs for regression instead of classification, the trick is to tweak the\n",
    "objective: instead of trying to fit the largest possible street between two\n",
    "classes while limiting margin violations, SVM regression tries to fit as many\n",
    "instances as possible on the street while limiting margin violations.\n",
    "\n",
    "The width of the street is controlled by a hyperparameter, ϵ. Reducing ϵ increases \n",
    "the number of support vectors, which regularizes the\n",
    "model. Moreover, if you add more training instances within the margin, it\n",
    "will not affect the model’s predictions; thus, the model is said to be ϵ-\n",
    "insensitive.\n",
    "\n",
    "To tackle nonlinear regression tasks, you can use a kernelized SVM model."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
