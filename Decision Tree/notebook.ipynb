{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "Decision trees are versatile machine learning algorithms that can perform\n",
    "both classification and regression tasks, and even multioutput tasks. They are\n",
    "powerful algorithms, capable of fitting complex datasets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "The way Decision Trees make predictions is so understandable. Suppose\n",
    "you find an iris flower and you want to classify it based on its petals. You\n",
    "start at the root node (depth 0, at the top): this node asks whether the flower’s\n",
    "petal length is smaller than 2.45 cm. If it is, then you move down to the root’s\n",
    "left child node (depth 1, left). In this case, it is a leaf node (i.e., it does not\n",
    "have any child nodes), so it does not ask any questions: simply look at the\n",
    "predicted class for that node, and the decision tree predicts that your flower is\n",
    "an Iris setosa (class=setosa).\n",
    "\n",
    "Now suppose you find another flower, and this time the petal length is greater\n",
    "than 2.45 cm. You again start at the root but now move down to its right child\n",
    "node (depth 1, right). This is not a leaf node, it’s a split node, so it asks\n",
    "another question: is the petal width smaller than 1.75 cm? If it is, then your\n",
    "flower is most likely an Iris versicolor (depth 2, left). If not, it is likely an Iris\n",
    "virginica (depth 2, right). It’s really that simple.\n",
    "\n",
    "A node’s samples attribute counts how many training instances it applies to.\n",
    "For example, 100 training instances have a petal length greater than 2.45 cm\n",
    "(depth 1, right), and of those 100, 54 have a petal width smaller than 1.75 cm\n",
    "(depth 2, left). A node’s value attribute tells you how many training instances\n",
    "of each class this node applies to: for example, the bottom-right node applies\n",
    "to 0 Iris setosa, 1 Iris versicolor, and 45 Iris virginica. Finally, a node’s gini\n",
    "attribute measures its Gini impurity: a node is “pure” (gini=0) if all training\n",
    "instances it applies to belong to the same class. For example, since the depth-\n",
    "1 left node applies only to Iris setosa training instances, it is pure and its Gini\n",
    "impurity is 0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation: White box versus black box\n",
    "\n",
    "Decision trees are intuitive, and their decisions are easy to interpret. Such\n",
    "models are often called white box models. In contrast, as you will see,\n",
    "random forests and neural networks are generally considered black box\n",
    "models. They make great predictions, and you can easily check the\n",
    "calculations that they performed to make these predictions; nevertheless,\n",
    "it is usually hard to explain in simple terms why the predictions were\n",
    "made. For example, if a neural network says that a particular person\n",
    "appears in a picture, it is hard to know what contributed to this prediction:\n",
    "Did the model recognize that person’s eyes? Their mouth? Their nose?\n",
    "Their shoes? Or even the couch that they were sitting on? Conversely,\n",
    "decision trees provide nice, simple classification rules that can even be applied manually if need be (e.g., for flower classification). The field of\n",
    "interpretable ML aims at creating ML systems that can explain their\n",
    "decisions in a way humans can understand. This is important in many\n",
    "domains—for example, to ensure the system does not make unfair\n",
    "decisions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Class Probabilities\n",
    "\n",
    "A decision tree can also estimate the probability that an instance belongs to a\n",
    "particular class k. First it traverses the tree to find the leaf node for this\n",
    "instance, and then it returns the ratio of training instances of class k in this\n",
    "node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The CART Training Algorithm\n",
    "\n",
    "The algorithm works by\n",
    "first splitting the training set into two subsets using a single feature k and a\n",
    "threshold t (e.g., “petal length ≤ 2.45 cm”). How does it choose k and t ? It\n",
    "searches for the pair (k, t ) that produces the purest subsets, weighted by their\n",
    "size.\n",
    "\n",
    "Once the CART algorithm has successfully split the training set in two, it\n",
    "splits the subsets using the same logic, then the sub-subsets, and so on,\n",
    "recursively. It stops recursing once it reaches the maximum depth (defined by\n",
    "the max_depth hyperparameter), or if it cannot find a split that will reduce\n",
    "impurity. A few other hyperparameters (described in a moment) control\n",
    "additional stopping conditions: min_samples_split, min_samples_leaf,\n",
    "min_weight_fraction_leaf, and max_leaf_nodes.\n",
    "\n",
    "As you can see, CART algorithm is a greedy algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Impurity or Entropy?\n",
    "\n",
    "By default, the DecisionTreeClassifier class uses the Gini impurity measure,\n",
    "but you can select the entropy impurity measure instead by setting the\n",
    "criterion hyperparameter to \"entropy\".\n",
    "\n",
    "\n",
    "\n",
    "Both Gini Impurity and Entropy are measures of impurity or uncertainty in a decision tree. Gini Impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. Entropy is a measure of the average amount of information contained in each item in the dataset.\n",
    "\n",
    "The formula for Gini Impurity is:\n",
    "\n",
    "\\begin{equation}\n",
    "I_G(p) = 1 - \\sum\\limits_{i=1}^J p_i^2\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ is the proportion of samples in the $i$-th class and $J$ is the total number of classes.\n",
    "\n",
    "The formula for Entropy is:\n",
    "\n",
    "$$\\begin{equation}\n",
    "I_H(p) = - \\sum\\limits_{i=1}^J p_i \\log_2(p_i)\n",
    "\\end{equation}$$\n",
    "\n",
    "where $p_i$ is the proportion of samples in the $i$-th class and $J$ is the total number of classes.\n",
    "\n",
    "In LaTeX, the formulas can be written as follows:\n",
    "\n",
    "Gini Impurity:\n",
    "\n",
    "\n",
    "$$I_G(p) = 1 - \\sum\\limits_{i=1}^J p_i^2$$\n",
    "\n",
    "\n",
    "Entropy:\n",
    "\n",
    "\n",
    "$$I_H(p) = - \\sum\\limits_{i=1}^J p_i \\log_2(p_i)$$\n",
    "\n",
    "\n",
    "So, should you use Gini impurity or entropy? The truth is, most of the time it\n",
    "does not make a big difference: they lead to similar trees. Gini impurity is\n",
    "slightly faster to compute, so it is a good default. However, when they differ,\n",
    "Gini impurity tends to isolate the most frequent class in its own branch of the\n",
    "tree, while entropy tends to produce slightly more balanced trees.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Hyperparameters\n",
    "\n",
    "Decision trees make very few assumptions about the training data (as\n",
    "opposed to linear models, which assume that the data is linear, for example).\n",
    "If left unconstrained, the tree structure will adapt itself to the training data,\n",
    "fitting it very closely—indeed, most likely overfitting it. Such a model is\n",
    "often called a nonparametric model, not because it does not have any\n",
    "parameters (it often has a lot) but because the number of parameters is not\n",
    "determined prior to training, so the model structure is free to stick closely to\n",
    "the data. In contrast, a parametric model, such as a linear model, has a\n",
    "predetermined number of parameters, so its degree of freedom is limited,\n",
    "reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "To avoid overfitting the training data, you need to restrict the decision tree’s\n",
    "freedom during training. As you know by now, this is called regularization.\n",
    "The regularization hyperparameters depend on the algorithm used, but\n",
    "generally you can at least restrict the maximum depth of the decision tree. In\n",
    "Scikit-Learn, this is controlled by the max_depth hyperparameter. The default\n",
    "value is None, which means unlimited. Reducing max_depth will regularize\n",
    "the model and thus reduce the risk of overfitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "The\n",
    "main difference is that instead of predicting a class in each node, it predicts a\n",
    "value. For example, suppose you want to make a prediction for a new\n",
    "instance with x = 0.2. The root node asks whether x ≤ 0.197. Since it is not,\n",
    "the algorithm goes to the right child node, which asks whether x ≤ 0.772.\n",
    "Since it is, the algorithm goes to the left child node. This is a leaf node, and it\n",
    "predicts value=0.111. This prediction is the average target value of the 110\n",
    "training instances associated with this leaf node, and it results in a mean\n",
    "squared error equal to 0.015 over these 110 instances.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations\n",
    "\n",
    "Desicion Trees are easy to understand and interpret, simple to use, versatile, and powerful. However, they have a few limitations. \n",
    "\n",
    "### Sensitivity to Axis Orientations\n",
    "\n",
    "First, as\n",
    "you may have noticed, decision trees love orthogonal decision boundaries (all\n",
    "splits are perpendicular to an axis), which makes them sensitive to the data’s\n",
    "orientation.\n",
    "\n",
    "### Decision Trees have a high variance\n",
    "\n",
    "More generally, the main issue with decision trees is that they have quite a\n",
    "high variance: small changes to the hyperparameters or to the data may\n",
    "produce very different models. Luckily, by averaging predictions over many trees, it’s possible to reduce\n",
    "variance significantly. Such an ensemble of trees is called a random forest."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
