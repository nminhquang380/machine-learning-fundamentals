{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Ensemble Learning and Random Forests\n",
    "\n",
    "Suppose you pose a complex question to thousands of random people, then\n",
    "aggregate their answers. In many cases you will find that this aggregated\n",
    "answer is better than an expert’s answer. This is called the wisdom of the\n",
    "crowd. Similarly, if you aggregate the predictions of a group of predictors\n",
    "(such as classifiers or regressors), you will often get better predictions than\n",
    "with the best individual predictor. A group of predictors is called an\n",
    "ensemble; thus, this technique is called **ensemble learning**, and an ensemble\n",
    "learning algorithm is called an **ensemble method**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifiers\n",
    "Suppose you have trained a few classifiers, each one achieving about 80%\n",
    "accuracy. You may have a logistic regression classifier, an SVM classifier, a\n",
    "random forest classifier, a k-nearest neighbors classifier, and perhaps a few\n",
    "more. A very simple way to create an even better classifier is to aggregate the\n",
    "predictions of each classifier: the class that gets the most votes is the\n",
    "ensemble’s prediction. This majority-vote classifier is called a hard voting\n",
    "classifier.\n",
    "\n",
    "Somewhat surprisingly, this voting classifier often achieves a higher accuracy\n",
    "than the best classifier in the ensemble. In fact, even if each classifier is a\n",
    "weak learner (meaning it does only slightly better than random guessing), the\n",
    "ensemble can still be a strong learner (achieving high accuracy), provided\n",
    "there are a sufficient number of weak learners in the ensemble and they are\n",
    "sufficiently diverse.\n",
    "\n",
    "If all classifiers are able to estimate class probabilities (i.e., if they all have a\n",
    "predict_proba() method), then you can tell Scikit-Learn to predict the class\n",
    "with the highest class probability, averaged over all the individual classifiers.\n",
    "This is called soft voting. It often achieves higher performance than hard\n",
    "voting because it gives more weight to highly confident votes. All you need\n",
    "to do is set the voting classifier’s voting hyperparameter to \"soft\", and ensure\n",
    "that all classifiers can estimate class probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[\n",
    "('lr', LogisticRegression(random_state=42)),\n",
    "('rf', RandomForestClassifier(random_state=42)),\n",
    "('svc', SVC(random_state=42))\n",
    "]\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Soft-voting\n",
    "voting_clf.voting = \"soft\"\n",
    "voting_clf.named_estimators[\"svc\"].probability = True\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_clf.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging and Pasting\n",
    "\n",
    "Another approach is to use the same training\n",
    "algorithm for every predictor but train them on different random subsets of\n",
    "the training set. When sampling is performed *with replacement*, this\n",
    "method is called **bagging** (short for bootstrap aggregating). When sampling\n",
    "is performed *without replacement*, it is called **pasting**.\n",
    "\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new\n",
    "instance by simply aggregating the predictions of all predictors. The\n",
    "aggregation function is typically the statistical mode for classification (i.e.,\n",
    "the most frequent prediction, just like with a hard voting classifier), or the\n",
    "average for regression. Each individual predictor has a higher bias than if it\n",
    "were trained on the original training set, but aggregation reduces both bias\n",
    "and variance. Generally, the net result is that the ensemble has a similar\n",
    "bias but a lower variance than a single predictor trained on the original\n",
    "training set.\n",
    "\n",
    "### Out-of-bag Evaluation\n",
    "\n",
    "With bagging, some training instances may be sampled several times for any\n",
    "given predictor, while others may not be sampled at all.  With this process, it\n",
    "can be shown mathematically that only about 63% of the training instances\n",
    "are sampled on average for each predictor. The remaining 37% of the\n",
    "training instances that are not sampled are called out-of-bag (OOB) instances.\n",
    "Note that they are not the same 37% for all predictors.\n",
    "\n",
    "A bagging ensemble can be evaluated using OOB instances, without the need\n",
    "for a separate validation set: indeed, if there are enough estimators, then each\n",
    "instance in the training set will likely be an OOB instance of several\n",
    "estimators, so these estimators can be used to make a fair ensemble prediction\n",
    "for that instance. Once you have a prediction for each instance, you can\n",
    "compute the ensemble’s prediction accuracy.\n",
    "\n",
    "### Random Patches and Random Subspaces\n",
    "\n",
    "The BaggingClassifier class supports sampling the features as well. Sampling\n",
    "is controlled by two hyperparameters: max_features and bootstrap_features.\n",
    "They work the same way as max_samples and bootstrap, but for feature\n",
    "sampling instead of instance sampling. Thus, each predictor will be trained\n",
    "on a random subset of the input features.\n",
    "\n",
    "This technique is particularly useful when you are dealing with \n",
    "high-dimensional inputs (such as images), as it can considerably speed up training.\n",
    "Sampling both training instances and features is called the **random patches\n",
    "method**. Keeping all training instances (by setting bootstrap=False and\n",
    "max_samples=1.0) but sampling features (by setting bootstrap_features to\n",
    "True and/or max_features to a value smaller than 1.0) is called the **random\n",
    "subspaces method**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "As we have discussed, a random forest is an ensemble of decision trees,\n",
    "generally trained via the bagging method (or sometimes pasting), typically\n",
    "with max_samples set to the size of the training set.\n",
    "\n",
    "The random forest algorithm introduces extra randomness when growing\n",
    "trees; instead of searching for the very best feature when splitting a node (see\n",
    "Chapter 6), it searches for the best feature among a random subset of features.\n",
    "By default, it samples n features (where n is the total number of features).\n",
    "The algorithm results in greater tree diversity, which (again) trades a higher\n",
    "bias for a lower variance, generally yielding an overall better model.\n",
    "\n",
    "### Feature importance\n",
    "\n",
    "Yet another great quality of random forests is that they make it easy to\n",
    "measure the relative importance of each feature. Scikit-Learn measures a\n",
    "feature’s importance by looking at how much the tree nodes that use that\n",
    "feature reduce impurity on average, across all trees in the forest. More\n",
    "precisely, it is a weighted average, where each node’s weight is equal to the\n",
    "number of training samples that are associated with it.\n",
    "\n",
    "Scikit-Learn computes this score automatically for each feature after training,\n",
    "then it scales the results so that the sum of all importances is equal to 1. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to any ensemble\n",
    "method that can combine several weak learners into a strong learner. The\n",
    "general idea of most boosting methods is to train predictors sequentially, each\n",
    "trying to correct its predecessor.\n",
    "\n",
    "### AdaBoost\n",
    "\n",
    "One way for a new predictor to correct its predecessor is to pay a bit more\n",
    "attention to the training instances that the predecessor underfit. This results in\n",
    "new predictors focusing more and more on the hard cases. This is the\n",
    "technique used by AdaBoost.\n",
    "\n",
    "For example, when training an AdaBoost classifier, the algorithm first trains\n",
    "a base classifier (such as a decision tree) and uses it to make predictions on\n",
    "the training set. The algorithm then increases the relative weight of\n",
    "misclassified training instances. Then it trains a second classifier, using the\n",
    "updated weights, and again makes predictions on the training set, updates the\n",
    "instance weights, and so on.\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Just like AdaBoost, gradient boosting works by sequentially adding predictors to an\n",
    "ensemble, each one correcting its predecessor. However, instead of tweaking\n",
    "the instance weights at every iteration like AdaBoost does, this method tries\n",
    "to fit the new predictor to the residual errors made by the previous predictor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking\n",
    "\n",
    "Stacking is based on a simple idea: instead of\n",
    "using trivial functions (such as hard voting) to aggregate the predictions of all\n",
    "predictors in an ensemble, why don’t we train a model to perform this\n",
    "aggregation?\n",
    "\n",
    "For more details, we have 3 predictors and each of them generates a different\n",
    "out for a task. Our mission is aggregate the predictions to get the final prediction.\n",
    "But, different from trivial function (hard voting, softvoting,..), we build an model\n",
    "which generate output from outputs of previous models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
