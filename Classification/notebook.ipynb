{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist.data, mnist.target\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_digit(image_data):\n",
    "    image = image_data.reshape(28,28)\n",
    "    plt.imshow(image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X[5]\n",
    "plot_digit(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plot_digit(X[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split trainset and testset\n",
    "The MNIST dataset returned by fetch_openml() is actually already split into a training set (the first 60,000 images) and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier\n",
    "Let’s simplify the problem for now and only try to identify one digit—for example, the number 5. This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and non-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == '5')\n",
    "y_test_5 = (y_test == '5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict([X[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measures"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s right, it has over 90% accuracy! This is simply because only about\n",
    "10% of the images are 5s, so if you always guess that an image is not a 5, you\n",
    "will be right about 90% of the time. Beats Nostradamus.\n",
    "This demonstrates why accuracy is generally not the preferred performance\n",
    "measure for classifiers, especially when you are dealing with ***skewed datasets***\n",
    "(i.e., when some classes are much more frequent than others). A much better\n",
    "way to evaluate the performance of a classifier is to look at the confusion\n",
    "matrix (CM)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices\n",
    "To compute the confusion matrix, you first need to have a set of predictions\n",
    "so that they can be compared to the actual targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the cross_val_score() function, cross_val_predict() performs k-fold\n",
    "cross-validation, but instead of returning the evaluation scores, it returns the\n",
    "predictions made on each test fold. This means that you get a clean prediction\n",
    "for each instance in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  confusion_matrix\n",
    "cm = confusion_matrix(y_train_5, y_train_pred)\n",
    "cm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in a confusion matrix represents an actual class, while each column\n",
    "represents a predicted class. The first row of this matrix considers non-5\n",
    "images (the **negative class**): 53,892 of them were correctly classified as non-\n",
    "5s (they are called ***true negatives***), while the remaining 687 were wrongly\n",
    "classified as 5s (***false positives***, also called type I errors). The second row\n",
    "considers the images of 5s (the **positive class**): 1,891 were wrongly classified\n",
    "as non-5s (***false negatives***, also called type II errors), while the remaining\n",
    "3,530 were correctly classified as 5s (***true positives***).\n",
    "> **|TN FP|** <br>\n",
    "> **|FN TP|**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "The confusion matrix gives you a lot of information, but sometimes you may\n",
    "prefer a more concise metric. An interesting one to look at is the accuracy of\n",
    "the positive predictions; this is called the **precision** of the classifier. <br>\n",
    "***precision = TP / (TP+FP)*** <br>\n",
    "Precision is typically used along with another metric named\n",
    "recall, also called sensitivity or the true positive rate (TPR): this is the ratio\n",
    "of positive instances that are correctly detected by the classifier. <br>\n",
    "***recall = TP / (TP + FN)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(y_train_5, y_train_pred), recall_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often convenient to combine precision and recall into a single metric\n",
    "called the F score, especially when you need a single metric to compare two\n",
    "classifiers. The F score is the harmonic mean of precision and recall. <br>\n",
    "F1 = 2*(1/precision + 1/recall) = 2*(precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Precision/Recall Trade-off\n",
    "For each instance, it computes a score based on a\n",
    "decision function. If that score is greater than a threshold.\n",
    "Scikit-Learn does not let you set the threshold directly, but it does give you\n",
    "access to the decision scores that it uses to make predictions. Instead of\n",
    "calling the classifier’s predict() method, you can call its decision_function()\n",
    "method, which returns a score for each instance, and then use any threshold\n",
    "you want to make predictions based on those scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function([X[0]])\n",
    "y_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decide which threshold to use? First, use the cross_val_predict()\n",
    "function to get the scores of all instances in the training set, but this time\n",
    "specify that you want to return decision scores instead of predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these scores, use the precision_recall_curve() function to compute\n",
    "precision and recall for all possible thresholds (the function adds a last\n",
    "precision of 0 and a last recall of 1, corresponding to an infinite threshold):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,3))\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.vlines(threshold, 0, 1.0, \"k\", \"dotted\", label=\"threshold\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.axis([-40000,40000,0,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_for_90_precision = (precisions >= 0.90).argmax()\n",
    "threshold_for_90_precision = thresholds[idx_for_90_precision]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROC Curve\n",
    "The receiver operating characteristic (ROC) curve is another common tool\n",
    "used with binary classifiers. It is very similar to the precision/recall curve, but\n",
    "instead of plotting precision versus recall, the ROC curve plots the true\n",
    "positive rate (another name for recall) against the false positive rate (FPR).\n",
    "The FPR (also called the fall-out) is the ratio of negative instances that are\n",
    "incorrectly classified as positive. It is equal to 1 – the true negative rate\n",
    "(TNR), which is the ratio of negative instances that are correctly classified as\n",
    "negative. The TNR is also called specificity. Hence, the ROC curve plots\n",
    "sensitivity (recall) versus 1 – specificity. <br>\n",
    "To plot the ROC curve, you first use the roc_curve() function to compute the\n",
    "TPR and FPR for various threshold values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "idx_for_threshold_at_90 = (thresholds <= threshold_for_90_precision).argmax()\n",
    "tpr_90, fpr_90 = tpr[idx_for_threshold_at_90], fpr[idx_for_threshold_at_90]\n",
    "plt.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "plt.plot([0, 1], [0, 1], 'k:', label=\"Random classifier's ROC curve\")\n",
    "plt.plot([fpr_90], [tpr_90], \"ko\", label=\"Threshold for 90% precision\")\n",
    "[...] # beautify the figure: add labels, grid, legend, arrow, and text\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5, y_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a RandomForestClassifier, which can compare with SGDClassifier in terms of PR curve and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_clf = RandomForestClassifier(random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision_recall_curve() function expects labels and scores for each\n",
    "instance, so we need to train the random forest classifier and make it assign a\n",
    "score to each instance. But the RandomForestClassifier class does not have a\n",
    "decision_function() method, due to the way it works. Luckily, it has a predict_proba() method that returns class\n",
    "probabilities for each instance, and we can just use the probability of the\n",
    "positive class as a score, so it will work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3,\n",
    "method=\"predict_proba\")\n",
    "y_probas_forest[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "precisions_forest, recalls_forest, thresholds_forest = precision_recall_curve(\n",
    "y_train_5, y_scores_forest)\n",
    "\n",
    "# Plot\n",
    "plt.plot(recalls_forest, precisions_forest, \"b-\", linewidth=2,\n",
    "label=\"Random Forest\")\n",
    "plt.plot(recalls, precisions, \"--\", linewidth=2, label=\"SGD\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now know how to train binary classifiers, choose the appropriate metric\n",
    "1\n",
    "for your task, evaluate your classifiers using cross-validation, select the\n",
    "precision/recall trade-off that fits your needs, and use several metrics and\n",
    "curves to compare various models. You’re ready to try to detect more than\n",
    "just the 5s."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "One way to create a system that can classify the digit images into 10 classes\n",
    "(from 0 to 9) is to train 10 binary classifiers, one for each digit (a 0-detector,\n",
    "a 1-detector, a 2-detector, and so on). Then when you want to classify an\n",
    "image, you get the decision score from each classifier for that image and you\n",
    "select the class whose classifier outputs the highest score. This is called the\n",
    "one-versus-the-rest (OvR) strategy, or sometimes one-versus-all (OvA). <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(random_state=42)\n",
    "svm_clf.fit(X_train[:2000], y_train[:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy! We trained the SVC using the original target classes from 0 to\n",
    "9 (y_train), instead of the 5-versus-the-rest target classes (y_train_5). Since\n",
    "there are 10 classes (i.e., more than 2), Scikit-Learn used the OvO strategy\n",
    "and trained 45 binary classifiers. Now let’s make a prediction on an image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.predict([X_train[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you call the\n",
    "decision_function() method, you will see that it returns 10 scores per\n",
    "instance: one per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores = svm_clf.decision_function([X_train[0]])\n",
    "some_digit_scores.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_digit_scores.argmax()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to force Scikit-Learn to use one-versus-one or one-versus-the￾rest, you can use the OneVsOneClassifier or OneVsRestClassifier classes.\n",
    "Simply create an instance and pass a classifier to its constructor (it doesn’t\n",
    "even have to be a binary classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "ovr_clf = OneVsRestClassifier(SVC(random_state=42))\n",
    "ovr_clf.fit(X_train[:2000], y_train[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ovr_clf.predict([X_train[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an SGDClassifier on a multiclass dataset and using it to make\n",
    "predictions is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "sgd_clf.predict([X_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train.astype(\"float64\"))\n",
    "cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "Here, we will assume that you have found a promising model and you want\n",
    "to find ways to improve it. One way to do this is to analyze the types of errors\n",
    "it makes. <br>\n",
    "First, look at the confusion matrix. For this, you first need to make\n",
    "predictions using the cross_val_predict() function; then you can pass the\n",
    "labels and predictions to the confusion_matrix() function, just like you did\n",
    "earlier. However, since there are now 10 classes instead of 2, the confusion\n",
    "matrix will contain quite a lot of numbers, and it may be hard to read.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
    "normalize=\"true\", values_format=\".0%\")\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the errors stand out more, we try putting zero weight on the correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = (y_train_pred != y_train)\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
    "sample_weight=sample_weight,\n",
    "normalize=\"pred\", values_format=\".0%\")\n",
    "ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,\n",
    "sample_weight=sample_weight,\n",
    "normalize=\"true\", values_format=\".0%\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilable Classification\n",
    "\n",
    "Until now, each instance has always been assigned to just one class. But in\n",
    "some cases you may want your classifier to output multiple classes for each\n",
    "instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_train_large = (y_train >= '7')\n",
    "y_train_odd = (y_train.astype('int8') % 2 == 1)\n",
    "y_multilabel = np.c_[y_train_large, y_train_odd]\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_multilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict([X_train[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to evaluate a multilabel classifier, and selecting the\n",
    "right metric really depends on your project. One approach is to measure the\n",
    "F score for each individual label (or any other binary classifier metric\n",
    "discussed earlier), then simply compute the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n",
    "f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach assumes that all labels are equally important, which may not\n",
    "be the case.  One simple option is to give each label a weight equal to its\n",
    "support (i.e., the number of instances with that target label). To do this,\n",
    "simply set average=\"weighted\" when calling the f1_score() function. <br><br>\n",
    "If you wish to use a classifier that does not natively support multilabel\n",
    "classification, such as SVC, one possible strategy is to train one model per\n",
    "label. However, this strategy may have a hard time capturing the\n",
    "dependencies between the labels. For example, a large digit (7, 8, or 9) is\n",
    "twice more likely to be odd than even, but the classifier for the “odd” label\n",
    "does not know what the classifier for the “large” label predicted. To solve this\n",
    "issue, the models can be organized in a chain: when a model makes a\n",
    "prediction, it uses the input features plus all the predictions of the models that\n",
    "come before it in the chain. <br><br>\n",
    "The good news is that Scikit-Learn has a class called ChainClassifier that\n",
    "does just that! By default it will use the true labels for training, feeding each\n",
    "model the appropriate labels depending on their position in the chain. But if\n",
    "you set the cv hyperparameter, it will use cross-validation to get “clean” (out-of-sample) predictions from each trained model for every instance in the\n",
    "training set, and these predictions will then be used to train all the models\n",
    "later in the chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import ClassifierChain\n",
    "chain_clf = ClassifierChain(SVC(), cv=3, random_state=42)\n",
    "chain_clf.fit(X_train[:2000], y_multilabel[:2000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multioutput Classification\n",
    "\n",
    "The last type of classification task we’ll discuss here is called multioutput–\n",
    "multiclass classification (or just multioutput classification). It is a\n",
    "generalization of multilabel classification where each label can be multiclass.\n",
    "\n",
    "To illustrate this, let’s build a system that removes noise from images. It will\n",
    "take as input a noisy digit image, and it will (hopefully) output a clean digit\n",
    "image, represented as an array of pixel intensities, just like the MNIST\n",
    "images. Notice that the classifier’s output is multilabel (one label per pixel)\n",
    "and each label can have multiple values (pixel intensity ranges from 0 to\n",
    "255). This is thus an example of a multioutput classification system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # to make this code example reproducible\n",
    "# Add noise to images\n",
    "noise = np.random.randint(0, 100, (len(X_train), 784))\n",
    "X_train_mod = X_train + noise\n",
    "noise = np.random.randint(0, 100, (len(X_test), 784))\n",
    "X_test_mod = X_test + noise\n",
    "# Now, the original images are the truth\n",
    "y_train_mod = X_train\n",
    "y_test_mod = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train_mod, y_train_mod)\n",
    "clean_digit = knn_clf.predict([X_test_mod[0]])\n",
    "plot_digit(clean_digit)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
